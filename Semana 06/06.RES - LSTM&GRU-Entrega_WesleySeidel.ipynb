{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alan-barzilay/NLPortugues/blob/master/imagens/logo_nlportugues.png?raw=true\"  style=\"height:65%\" align=\"right\">\n",
    "\n",
    "\n",
    "# Lista 6 - LSTM&GRU \n",
    "**Nome:** Wesley Seidel Carvalho\n",
    "\n",
    "**Numero Usp:** 6544342\n",
    "\n",
    "______________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O objetivo desta lista é fazer com que vocês treinem um modelo de análise de sentimentos utilizando GRU's e LSTM's. Essa lista é semelhante a lista 03 onde aprendemos a usar embeddings e onde você ja recebeu a arquitetura do seu modelo quase pronta. A diferença é que desta vez você ira construir sozinho sua rede e utilizará as camadas que acabamos de aprender: LSTM e GRU.\n",
    " \n",
    "Essa tambêm será a primeira rede recorrente que montaremos, portanto a tokenização será ligeiramente diferente (por exemplo o padding não é mais necessário.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-rc3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n",
    "# '2.2.0-rc3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os dados como um dataframe\n",
    "\n",
    "Para esta lista nós utilizaremos um recorte de 10 mil linhas do dataset **B2W-Reviews01** que consiste em avaliações de mais de 130k compras online no site Americanas.com e [esta disponivel no github](https://github.com/b2wdigital/b2w-reviews01) sob a licensa CC BY-NC-SA 4.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>site_category_lv1</th>\n",
       "      <th>site_category_lv2</th>\n",
       "      <th>review_title</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>recommend_to_a_friend</th>\n",
       "      <th>review_text</th>\n",
       "      <th>reviewer_birth_year</th>\n",
       "      <th>reviewer_gender</th>\n",
       "      <th>reviewer_state</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:11:28</td>\n",
       "      <td>d0fb1ca69422530334178f5c8624aa7a99da47907c44de...</td>\n",
       "      <td>132532965</td>\n",
       "      <td>Notebook Asus Vivobook Max X541NA-GO472T Intel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Informática</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>Bom</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>1958</td>\n",
       "      <td>F</td>\n",
       "      <td>RJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:13:48</td>\n",
       "      <td>014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...</td>\n",
       "      <td>22562178</td>\n",
       "      <td>Copo Acrílico Com Canudo 500ml Rocie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Utilidades Domésticas</td>\n",
       "      <td>Copos, Taças e Canecas</td>\n",
       "      <td>Preço imbatível, ótima qualidade</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>1996</td>\n",
       "      <td>M</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:26:02</td>\n",
       "      <td>44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...</td>\n",
       "      <td>113022329</td>\n",
       "      <td>Panela de Pressão Elétrica Philips Walita Dail...</td>\n",
       "      <td>philips walita</td>\n",
       "      <td>Eletroportáteis</td>\n",
       "      <td>Panela Elétrica</td>\n",
       "      <td>ATENDE TODAS AS EXPECTATIVA.</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>1984</td>\n",
       "      <td>M</td>\n",
       "      <td>SP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:35:54</td>\n",
       "      <td>ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...</td>\n",
       "      <td>113851581</td>\n",
       "      <td>Betoneira Columbus - Roma Brinquedos</td>\n",
       "      <td>roma jensen</td>\n",
       "      <td>Brinquedos</td>\n",
       "      <td>Veículos de Brinquedo</td>\n",
       "      <td>presente mais que desejado</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>1985</td>\n",
       "      <td>F</td>\n",
       "      <td>SP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 01:00:28</td>\n",
       "      <td>7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...</td>\n",
       "      <td>131788803</td>\n",
       "      <td>Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...</td>\n",
       "      <td>lg</td>\n",
       "      <td>TV e Home Theater</td>\n",
       "      <td>TV</td>\n",
       "      <td>Sem duvidas, excelente</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>1994</td>\n",
       "      <td>M</td>\n",
       "      <td>MG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       submission_date                                        reviewer_id  \\\n",
       "0  2018-01-01 00:11:28  d0fb1ca69422530334178f5c8624aa7a99da47907c44de...   \n",
       "1  2018-01-01 00:13:48  014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...   \n",
       "2  2018-01-01 00:26:02  44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...   \n",
       "3  2018-01-01 00:35:54  ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...   \n",
       "4  2018-01-01 01:00:28  7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...   \n",
       "\n",
       "   product_id                                       product_name  \\\n",
       "0   132532965  Notebook Asus Vivobook Max X541NA-GO472T Intel...   \n",
       "1    22562178               Copo Acrílico Com Canudo 500ml Rocie   \n",
       "2   113022329  Panela de Pressão Elétrica Philips Walita Dail...   \n",
       "3   113851581               Betoneira Columbus - Roma Brinquedos   \n",
       "4   131788803  Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...   \n",
       "\n",
       "    product_brand      site_category_lv1       site_category_lv2  \\\n",
       "0             NaN            Informática                Notebook   \n",
       "1             NaN  Utilidades Domésticas  Copos, Taças e Canecas   \n",
       "2  philips walita        Eletroportáteis         Panela Elétrica   \n",
       "3     roma jensen             Brinquedos   Veículos de Brinquedo   \n",
       "4              lg      TV e Home Theater                      TV   \n",
       "\n",
       "                       review_title  overall_rating recommend_to_a_friend  \\\n",
       "0                               Bom               4                   Yes   \n",
       "1  Preço imbatível, ótima qualidade               4                   Yes   \n",
       "2      ATENDE TODAS AS EXPECTATIVA.               4                   Yes   \n",
       "3        presente mais que desejado               4                   Yes   \n",
       "4            Sem duvidas, excelente               5                   Yes   \n",
       "\n",
       "                                         review_text reviewer_birth_year  \\\n",
       "0  Estou contente com a compra entrega rápida o ú...                1958   \n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...                1996   \n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...                1984   \n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...                1985   \n",
       "4  A entrega foi no prazo, as americanas estão de...                1994   \n",
       "\n",
       "  reviewer_gender reviewer_state Unnamed: 14 Unnamed: 15 Unnamed: 16  \\\n",
       "0               F             RJ         NaN         NaN         NaN   \n",
       "1               M             SC         NaN         NaN         NaN   \n",
       "2               M             SP         NaN         NaN         NaN   \n",
       "3               F             SP         NaN         NaN         NaN   \n",
       "4               M             MG         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 17 Unnamed: 18  \n",
       "0         NaN         NaN  \n",
       "1         NaN         NaN  \n",
       "2         NaN         NaN  \n",
       "3         NaN         NaN  \n",
       "4         NaN         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus = pd.read_csv(\"../data/b2w-10k.csv\")\n",
    "b2wCorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Estou contente com a compra entrega rápida o ú...\n",
       "1       Por apenas R$1994.20,eu consegui comprar esse ...\n",
       "2       SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...\n",
       "3       MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...\n",
       "4       A entrega foi no prazo, as americanas estão de...\n",
       "                              ...                        \n",
       "9994    Celular muito rápido, com processador e armaze...\n",
       "9995    achei o produto muito frágil, o material veio ...\n",
       "9996    Uma porcaria pois ñ recebi ñ recomendo pra nin...\n",
       "9997    Maquina excelente,super pratica. recomendo.ent...\n",
       "9998    Agradeço pelo compromisso, obrigado. ,...........\n",
       "Name: review_text, Length: 9999, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus[\"review_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pré-processamento \n",
    "# <font color='blue'>Questão 1 </font>\n",
    "Copie suas etapas de préprocessamento da lista 03, ou seja, selecione apenas as colunas relevantes (\"review_text\" e \"recommend_to_a_friend\"), converta a coluna \"review_text\" de uma coluna de `str` para uma coluna de `int` e separe os dados em teste e treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = b2wCorpus[['review_text','recommend_to_a_friend']]\n",
    "df = df.assign(recommend_to_a_friend_new=0)\n",
    "df['recommend_to_a_friend_new'] = df.recommend_to_a_friend.map({'Yes': 1, 'No': 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review_text'], df['recommend_to_a_friend_new'], test_size=0.20, random_state=17)\n",
    "\n",
    "# df.groupby('recommend_to_a_friend').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizando\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Questão 2 </font>\n",
    "Utilizando a camada [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) tokenize os inputs.\n",
    "Declare a camada e então chame a função `adapt()` no seu conjunto de treino para adequar o seu vocabulário aos reviews. \n",
    "\n",
    "Note que o uso de padding não é mais necessario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "# Estou inicializando algumas variaveis aqui que serão utilizadas nas demais questões.\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SENTENCE_LENGTH = 75\n",
    "EMBEDDING_LENGTH = 128\n",
    "QNT_EPOCAS_TREINO = 15\n",
    "\n",
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=MAX_SENTENCE_LENGTH,  # Only valid in INT mode.\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM&GRU\n",
    "\n",
    "Agora vamos juntar a camada do tokenizador a nossa camada [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) e definir o resto de nosso modelo.\n",
    "\n",
    "#  <font color='blue'>Questão 3 </font>\n",
    "\n",
    "a) Defina, compile, treine e avalie seu modelo, utilize camadas  [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/).\n",
    "Atenção a dimensão do input da camada de embedding, lembre se que < OOV > e < PAD > possuem seus próprios tokens.\n",
    " \n",
    " \n",
    " \n",
    "b) Como foi a performance desta rede em comparação a da lista 3?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Sua resposta aqui </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 0.5525 - acc: 0.7568\n",
      "Epoch 2/15\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 0.5018 - acc: 0.7936\n",
      "Epoch 3/15\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 0.4745 - acc: 0.8206\n",
      "Epoch 4/15\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 0.4614 - acc: 0.8272\n",
      "Epoch 5/15\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.4690 - acc: 0.8244\n",
      "Epoch 6/15\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.4565 - acc: 0.8287\n",
      "Epoch 7/15\n",
      "250/250 [==============================] - 23s 91ms/step - loss: 0.4327 - acc: 0.8294\n",
      "Epoch 8/15\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.4195 - acc: 0.8397\n",
      "Epoch 9/15\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.3605 - acc: 0.8435\n",
      "Epoch 10/15\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.3230 - acc: 0.8725\n",
      "Epoch 11/15\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.2960 - acc: 0.8867\n",
      "Epoch 12/15\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.3019 - acc: 0.8891\n",
      "Epoch 13/15\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 0.3013 - acc: 0.8954\n",
      "Epoch 14/15\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 0.2417 - acc: 0.9146\n",
      "Epoch 15/15\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 0.1911 - acc: 0.9379\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 75, 128)           640128    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 771,841\n",
      "Trainable params: 771,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.3117 - acc: 0.8795\n",
      "Loss:  0.31173521280288696\n",
      "Accuracy:  0.8794999718666077\n"
     ]
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE+1, output_dim=EMBEDDING_LENGTH, input_length=MAX_SENTENCE_LENGTH),\n",
    "    tf.keras.layers.LSTM(\n",
    "        units=EMBEDDING_LENGTH, \n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "    ),\n",
    "\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4, seed=random.randint(0,15)),\n",
    "    \n",
    "    layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "\n",
    "model.fit(X_train, y_train,epochs=QNT_EPOCAS_TREINO)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss, accuracy = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# b)\n",
    "Usando as mesmas seguintes configurações para os exercicios desta lista e da lista 3:\n",
    "- VOCAB_SIZE = 5000\n",
    "- MAX_SENTENCE_LENGTH = 75\n",
    "- EMBEDDING_LENGTH = 128\n",
    "- QNT_EPOCAS_TREINO = 15\n",
    "\n",
    "\n",
    "Enquanto na lista 3 tivemos os seguintes valores:\n",
    "- Loss:  1.1264379024505615\n",
    "- Accuracy:  0.8845000267028809\n",
    "\n",
    "\n",
    "E agora usando o LSTM conseguimos:\n",
    "- Loss:  0.31173521280288696\n",
    "- Accuracy:  0.8794999718666077\n",
    "\n",
    "Apesar da Acurácia se apresentar menor com o LSTM, o LOSS foi bem menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='blue'>Questão 4 </font>\n",
    "\n",
    "a) Defina, compile, treine e avalie seu modelo, utilize camadas [GRU](https://keras.io/api/layers/recurrent_layers/gru/).\n",
    "Atenção a dimensão do input da camada de embedding, lembre se que < OOV > e < PAD > possuem seus próprios tokens.\n",
    " \n",
    " \n",
    " \n",
    "b) Como foi a performance desta rede em comparação a da lista 3?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Sua resposta aqui </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.5645 - acc: 0.7465\n",
      "Epoch 2/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.5033 - acc: 0.7797\n",
      "Epoch 3/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.2396 - acc: 0.9111\n",
      "Epoch 4/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.1542 - acc: 0.9496\n",
      "Epoch 5/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.1126 - acc: 0.9670\n",
      "Epoch 6/15\n",
      "250/250 [==============================] - 17s 67ms/step - loss: 0.0799 - acc: 0.9789\n",
      "Epoch 7/15\n",
      "250/250 [==============================] - 17s 67ms/step - loss: 0.0622 - acc: 0.9835\n",
      "Epoch 8/15\n",
      "250/250 [==============================] - 17s 67ms/step - loss: 0.0511 - acc: 0.9866\n",
      "Epoch 9/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.0405 - acc: 0.9897\n",
      "Epoch 10/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.0329 - acc: 0.9916\n",
      "Epoch 11/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.0272 - acc: 0.9936\n",
      "Epoch 12/15\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.0239 - acc: 0.9939\n",
      "Epoch 13/15\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0233 - acc: 0.9936\n",
      "Epoch 14/15\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 0.0219 - acc: 0.9939\n",
      "Epoch 15/15\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 0.0163 - acc: 0.9959\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 75, 128)           640128    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 739,329\n",
      "Trainable params: 739,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.6628 - acc: 0.8635\n",
      "Loss:  0.6627704501152039\n",
      "Accuracy:  0.8634999990463257\n"
     ]
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE+1, output_dim=EMBEDDING_LENGTH, input_length=MAX_SENTENCE_LENGTH),\n",
    "    tf.keras.layers.GRU(\n",
    "        units=EMBEDDING_LENGTH, \n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "    ),\n",
    "    layers.Dropout(0.4, seed=random.randint(0,15)),\n",
    "    layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "\n",
    "model.fit(X_train, y_train,epochs=QNT_EPOCAS_TREINO)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss, accuracy = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# b)\n",
    "Usando as mesmas seguintes configurações para os exercicios desta lista e da lista 3:\n",
    "- VOCAB_SIZE = 5000\n",
    "- MAX_SENTENCE_LENGTH = 75\n",
    "- EMBEDDING_LENGTH = 128\n",
    "- QNT_EPOCAS_TREINO = 15\n",
    "\n",
    "\n",
    "Enquanto na lista 3 tivemos os seguintes valores:\n",
    "- Loss:  1.1264379024505615\n",
    "- Accuracy:  0.8845000267028809\n",
    "\n",
    "\n",
    "E agora usando o GRU conseguimos:\n",
    "- Loss:  0.6627704501152039\n",
    "- Accuracy:  0.8634999990463257\n",
    "\n",
    "Apesar da Acurácia se apresentar menor com o GRU, o LOSS foi menor, mas não foi menor do que o LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Bi-direcionais\n",
    "#  <font color='blue'>Questão 5 </font>\n",
    "\n",
    "a) Defina, compile, treine e avalie um novo modelo que utilize contexto em ambas as direções usando a camada [`Bidirectional()`](https://keras.io/api/layers/recurrent_layers/bidirectional/), seja com camadas GRU ou LSTM.\n",
    "\n",
    "\n",
    "b) Como foi sua performance em relação as questões anteriores com contexto unidirecional?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Sua resposta aqui </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 28.9786 - acc: 0.7950\n",
      "Epoch 2/15\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.5989 - acc: 0.8565\n",
      "Epoch 3/15\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3107 - acc: 0.8632\n",
      "Epoch 4/15\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 2342315776.0000 - acc: 0.8712\n",
      "Epoch 5/15\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 1.3304 - acc: 0.8749\n",
      "Epoch 6/15\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3889 - acc: 0.8806\n",
      "Epoch 7/15\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 0.3220 - acc: 0.8799\n",
      "Epoch 8/15\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3336 - acc: 0.8642\n",
      "Epoch 9/15\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.3218 - acc: 0.8747\n",
      "Epoch 10/15\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3000 - acc: 0.8891\n",
      "Epoch 11/15\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 0.2786 - acc: 0.8876\n",
      "Epoch 12/15\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.2695 - acc: 0.8965\n",
      "Epoch 13/15\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.2388 - acc: 0.9074\n",
      "Epoch 14/15\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 2.7374 - acc: 0.9101\n",
      "Epoch 15/15\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 13.0323 - acc: 0.9039\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 75, 128)           640128    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 903,553\n",
      "Trainable params: 903,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "63/63 [==============================] - 3s 41ms/step - loss: 0.6583 - acc: 0.8670\n",
      "Loss:  0.6583318114280701\n",
      "Accuracy:  0.8669999837875366\n"
     ]
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE+1, output_dim=EMBEDDING_LENGTH, input_length=MAX_SENTENCE_LENGTH),\n",
    "\n",
    "    # Camada Bidirecional\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(units=EMBEDDING_LENGTH, activation=\"tanh\", recurrent_activation=\"sigmoid\"),\n",
    "        backward_layer=tf.keras.layers.LSTM(units=EMBEDDING_LENGTH, activation=\"relu\", recurrent_activation=\"sigmoid\", go_backwards=True)\n",
    "    ),   \n",
    "    \n",
    "    layers.Dropout(0.4, seed=random.randint(0,15)),\n",
    "    layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "\n",
    "model.fit(X_train, y_train,epochs=QNT_EPOCAS_TREINO)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss, accuracy = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) \n",
    "Usando as mesmas seguintes configurações para os exercicios desta lista e da lista 3:\n",
    "- VOCAB_SIZE = 5000\n",
    "- MAX_SENTENCE_LENGTH = 75\n",
    "- EMBEDDING_LENGTH = 128\n",
    "- QNT_EPOCAS_TREINO = 15\n",
    "\n",
    "\n",
    "Enquanto na lista 3 tivemos os seguintes valores:\n",
    "- Loss:  1.1264379024505615\n",
    "- Accuracy:  0.8845000267028809\n",
    "\n",
    "\n",
    "E agora usando o GRU conseguimos:\n",
    "- Loss:  0.6583318114280701\n",
    "- Accuracy:  0.8669999837875366\n",
    "\n",
    "Neste exemplo a Acurácia também se apresentou menor com relação ao exercicio da lista 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comentários extras:\n",
    "\n",
    "Além das configurações acima comentadas, também executei na lista 3 as seguintes configurações 'fixas':\n",
    "- VOCAB_SIZE = 5000\n",
    "- EMBEDDING_LENGTH = 128\n",
    "- Otimizador: 'adam'\n",
    "- Função de perda: binary_crossentropy\n",
    "\n",
    "E diferentes valores para o tamanho da sentença e épocas de treinamento, que seguem:\n",
    "\n",
    "| MAX_SENTENCE_LENGTH | EPOCAS | Loss | Accuracy |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "|  30 | 15 | 1.293 | 0.874 |\n",
    "|  50 | 15 | 1.520 | 0.877 |\n",
    "|  63 | 15 | 1.396 | 0.881 |\n",
    "|  75 | 15 | 1.126 | 0.884 |\n",
    "| 100 | 15 | 1.188 | 0.887 |\n",
    "| 100 | 50 | 1.863 | 0.870 |\n",
    "\n",
    "\n",
    "Além dissom realizei também as seguintes configurações do GRU, LSTM e Biderectional utilizando os seguintes valores fixos:\n",
    "- VOCAB_SIZE = 5000\n",
    "- EMBEDDING_LENGTH = 128\n",
    "- Otimizador: 'adam'\n",
    "- Função de perda: binary_crossentropy\n",
    "\n",
    "e obtendo os seguintes resultados com as algumas variações de configuração:\n",
    "\n",
    "| Descrição   | MAX_SENTENCE_LENGTH   | EPOCAS | Loss   | Accuracy |\n",
    "| ---------   | ------                | ------ | ------ | ------   |\n",
    "| LSTM + Dropout 0.4         |  75    | 15     | 0.311  | 0.879    |\n",
    "| GRU  + Dropout 0.4         |  75    | 15     | 0.662  | 0.863    |\n",
    "| Bidirecional + Dropout 0.4 |  75    | 15     | 0.658  | 0.867    |\n",
    "| ---------   | ------                | ------ | ------ | ------   |\n",
    "| LSTM + Dropout 0.4         |  100   | 50     | 0.813  | 0.870    |\n",
    "| GRU  + Dropout 0.4         |  100   | 50     | 1.081  | 0.865    |\n",
    "| Bidirecional + Dropout 0.4 |  100   | 50     | 7.961  | 0.869    |\n",
    "| ---------   | ------                | ------ | ------ | ------   |\n",
    "| LSTM                       |  100   | 50     | 0.719  | 0.874    |\n",
    "| GRU                        |  100   | 50     | 0.831  | 0.878    |\n",
    "| Bidirecional               |  100   | 50     | 1.152  | 0.855    |\n",
    "| ---------   | ------                | ------ | ------ | ------   |\n",
    "\n",
    "Onde se lê \"XXX + Dropout 0.4\", entenda-se como: \"Modelo XXX usando uma camada posterior de Dropout em 0.4\".\n",
    "\n",
    "\n",
    "Com as configurações testados, não observei melhorias na acurácia utilizando as técnicas GRU, LSTM e Bidirecional em comparação com as configurações da lista 3. Acredito que valha a pena testar diferentes otimizadores no treinamento para efeito de comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
