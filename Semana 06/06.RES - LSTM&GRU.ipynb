{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alan-barzilay/NLPortugues/blob/master/imagens/logo_nlportugues.png?raw=true\"  style=\"height:65%\" align=\"right\">\n",
    "\n",
    "\n",
    "# Lista 6 - LSTM&GRU \n",
    "**Nome:** Wesley Seidel Carvalho\n",
    "\n",
    "**Numero Usp:** 6544342\n",
    "\n",
    "______________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O objetivo desta lista é fazer com que vocês treinem um modelo de análise de sentimentos utilizando GRU's e LSTM's. Essa lista é semelhante a lista 03 onde aprendemos a usar embeddings e onde você ja recebeu a arquitetura do seu modelo quase pronta. A diferença é que desta vez você ira construir sozinho sua rede e utilizará as camadas que acabamos de aprender: LSTM e GRU.\n",
    " \n",
    "Essa tambêm será a primeira rede recorrente que montaremos, portanto a tokenização será ligeiramente diferente (por exemplo o padding não é mais necessário.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-rc3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n",
    "# '2.2.0-rc3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os dados como um dataframe\n",
    "\n",
    "Para esta lista nós utilizaremos um recorte de 10 mil linhas do dataset **B2W-Reviews01** que consiste em avaliações de mais de 130k compras online no site Americanas.com e [esta disponivel no github](https://github.com/b2wdigital/b2w-reviews01) sob a licensa CC BY-NC-SA 4.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>site_category_lv1</th>\n",
       "      <th>site_category_lv2</th>\n",
       "      <th>review_title</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>recommend_to_a_friend</th>\n",
       "      <th>review_text</th>\n",
       "      <th>reviewer_birth_year</th>\n",
       "      <th>reviewer_gender</th>\n",
       "      <th>reviewer_state</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:11:28</td>\n",
       "      <td>d0fb1ca69422530334178f5c8624aa7a99da47907c44de...</td>\n",
       "      <td>132532965</td>\n",
       "      <td>Notebook Asus Vivobook Max X541NA-GO472T Intel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Informática</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>Bom</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>1958</td>\n",
       "      <td>F</td>\n",
       "      <td>RJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:13:48</td>\n",
       "      <td>014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...</td>\n",
       "      <td>22562178</td>\n",
       "      <td>Copo Acrílico Com Canudo 500ml Rocie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Utilidades Domésticas</td>\n",
       "      <td>Copos, Taças e Canecas</td>\n",
       "      <td>Preço imbatível, ótima qualidade</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>1996</td>\n",
       "      <td>M</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:26:02</td>\n",
       "      <td>44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...</td>\n",
       "      <td>113022329</td>\n",
       "      <td>Panela de Pressão Elétrica Philips Walita Dail...</td>\n",
       "      <td>philips walita</td>\n",
       "      <td>Eletroportáteis</td>\n",
       "      <td>Panela Elétrica</td>\n",
       "      <td>ATENDE TODAS AS EXPECTATIVA.</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>1984</td>\n",
       "      <td>M</td>\n",
       "      <td>SP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:35:54</td>\n",
       "      <td>ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...</td>\n",
       "      <td>113851581</td>\n",
       "      <td>Betoneira Columbus - Roma Brinquedos</td>\n",
       "      <td>roma jensen</td>\n",
       "      <td>Brinquedos</td>\n",
       "      <td>Veículos de Brinquedo</td>\n",
       "      <td>presente mais que desejado</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>1985</td>\n",
       "      <td>F</td>\n",
       "      <td>SP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 01:00:28</td>\n",
       "      <td>7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...</td>\n",
       "      <td>131788803</td>\n",
       "      <td>Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...</td>\n",
       "      <td>lg</td>\n",
       "      <td>TV e Home Theater</td>\n",
       "      <td>TV</td>\n",
       "      <td>Sem duvidas, excelente</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>1994</td>\n",
       "      <td>M</td>\n",
       "      <td>MG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       submission_date                                        reviewer_id  \\\n",
       "0  2018-01-01 00:11:28  d0fb1ca69422530334178f5c8624aa7a99da47907c44de...   \n",
       "1  2018-01-01 00:13:48  014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...   \n",
       "2  2018-01-01 00:26:02  44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...   \n",
       "3  2018-01-01 00:35:54  ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...   \n",
       "4  2018-01-01 01:00:28  7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...   \n",
       "\n",
       "   product_id                                       product_name  \\\n",
       "0   132532965  Notebook Asus Vivobook Max X541NA-GO472T Intel...   \n",
       "1    22562178               Copo Acrílico Com Canudo 500ml Rocie   \n",
       "2   113022329  Panela de Pressão Elétrica Philips Walita Dail...   \n",
       "3   113851581               Betoneira Columbus - Roma Brinquedos   \n",
       "4   131788803  Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...   \n",
       "\n",
       "    product_brand      site_category_lv1       site_category_lv2  \\\n",
       "0             NaN            Informática                Notebook   \n",
       "1             NaN  Utilidades Domésticas  Copos, Taças e Canecas   \n",
       "2  philips walita        Eletroportáteis         Panela Elétrica   \n",
       "3     roma jensen             Brinquedos   Veículos de Brinquedo   \n",
       "4              lg      TV e Home Theater                      TV   \n",
       "\n",
       "                       review_title  overall_rating recommend_to_a_friend  \\\n",
       "0                               Bom               4                   Yes   \n",
       "1  Preço imbatível, ótima qualidade               4                   Yes   \n",
       "2      ATENDE TODAS AS EXPECTATIVA.               4                   Yes   \n",
       "3        presente mais que desejado               4                   Yes   \n",
       "4            Sem duvidas, excelente               5                   Yes   \n",
       "\n",
       "                                         review_text reviewer_birth_year  \\\n",
       "0  Estou contente com a compra entrega rápida o ú...                1958   \n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...                1996   \n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...                1984   \n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...                1985   \n",
       "4  A entrega foi no prazo, as americanas estão de...                1994   \n",
       "\n",
       "  reviewer_gender reviewer_state Unnamed: 14 Unnamed: 15 Unnamed: 16  \\\n",
       "0               F             RJ         NaN         NaN         NaN   \n",
       "1               M             SC         NaN         NaN         NaN   \n",
       "2               M             SP         NaN         NaN         NaN   \n",
       "3               F             SP         NaN         NaN         NaN   \n",
       "4               M             MG         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 17 Unnamed: 18  \n",
       "0         NaN         NaN  \n",
       "1         NaN         NaN  \n",
       "2         NaN         NaN  \n",
       "3         NaN         NaN  \n",
       "4         NaN         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus = pd.read_csv(\"../data/b2w-10k.csv\")\n",
    "b2wCorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Estou contente com a compra entrega rápida o ú...\n",
       "1       Por apenas R$1994.20,eu consegui comprar esse ...\n",
       "2       SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...\n",
       "3       MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...\n",
       "4       A entrega foi no prazo, as americanas estão de...\n",
       "                              ...                        \n",
       "9994    Celular muito rápido, com processador e armaze...\n",
       "9995    achei o produto muito frágil, o material veio ...\n",
       "9996    Uma porcaria pois ñ recebi ñ recomendo pra nin...\n",
       "9997    Maquina excelente,super pratica. recomendo.ent...\n",
       "9998    Agradeço pelo compromisso, obrigado. ,...........\n",
       "Name: review_text, Length: 9999, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus[\"review_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pré-processamento \n",
    "# <font color='blue'>Questão 1 </font>\n",
    "Copie suas etapas de préprocessamento da lista 03, ou seja, selecione apenas as colunas relevantes (\"review_text\" e \"recommend_to_a_friend\"), converta a coluna \"review_text\" de uma coluna de `str` para uma coluna de `int` e separe os dados em teste e treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = b2wCorpus[['review_text','recommend_to_a_friend']]\n",
    "df = df.assign(recommend_to_a_friend_new=0)\n",
    "df['recommend_to_a_friend_new'] = df.recommend_to_a_friend.map({'Yes': 1, 'No': 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review_text'], df['recommend_to_a_friend_new'], test_size=0.20, random_state=17)\n",
    "\n",
    "# df.groupby('recommend_to_a_friend').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizando\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Questão 2 </font>\n",
    "Utilizando a camada [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) tokenize os inputs.\n",
    "Declare a camada e então chame a função `adapt()` no seu conjunto de treino para adequar o seu vocabulário aos reviews. \n",
    "\n",
    "Note que o uso de padding não é mais necessario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "# Estou inicializando algumas variaveis aqui que serão utilizadas nas demais questões.\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SENTENCE_LENGTH = 100\n",
    "EMBEDDING_LENGTH = 128\n",
    "QNT_EPOCAS_TREINO = 50\n",
    "\n",
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=MAX_SENTENCE_LENGTH,  # Only valid in INT mode.\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM&GRU\n",
    "\n",
    "Agora vamos juntar a camada do tokenizador a nossa camada [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) e definir o resto de nosso modelo.\n",
    "\n",
    "#  <font color='blue'>Questão 3 </font>\n",
    "\n",
    "a) Defina, compile, treine e avalie seu modelo, utilize camadas  [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/).\n",
    "Atenção a dimensão do input da camada de embedding, lembre se que < OOV > e < PAD > possuem seus próprios tokens.\n",
    " \n",
    " \n",
    " \n",
    "b) Como foi a performance desta rede em comparação a da lista 3?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Sua resposta aqui </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.5677 - acc: 0.7483\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 65s 261ms/step - loss: 0.5488 - acc: 0.7431\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 68s 274ms/step - loss: 0.5481 - acc: 0.7486\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.5591 - acc: 0.7547\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 0.5551 - acc: 0.7563\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 29s 118ms/step - loss: 0.5474 - acc: 0.7538\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.5014 - acc: 0.7807\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 33s 130ms/step - loss: 0.4044 - acc: 0.8472\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 0.3179 - acc: 0.8795\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.2478 - acc: 0.9146\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 32s 130ms/step - loss: 0.1946 - acc: 0.9342\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.1598 - acc: 0.9522\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.1277 - acc: 0.9639\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.1116 - acc: 0.9716\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.1049 - acc: 0.9722\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.0843 - acc: 0.9801\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.0790 - acc: 0.9821\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.0734 - acc: 0.9841\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.0638 - acc: 0.9870\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.0646 - acc: 0.9866\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.0619 - acc: 0.9874\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.0571 - acc: 0.9889\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 0.0567 - acc: 0.9892\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 32s 130ms/step - loss: 0.0539 - acc: 0.9890\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.0552 - acc: 0.9880\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.0512 - acc: 0.9894\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.0466 - acc: 0.9904\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.0464 - acc: 0.9887\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 0.0384 - acc: 0.9914\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.0341 - acc: 0.9917\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 29s 116ms/step - loss: 0.0324 - acc: 0.9930\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.0359 - acc: 0.9916\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.0517 - acc: 0.9866\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 0.0368 - acc: 0.9900\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0293 - acc: 0.9930\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.0257 - acc: 0.9941\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.0241 - acc: 0.9947\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0211 - acc: 0.9954\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.0205 - acc: 0.9954\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0194 - acc: 0.9959\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0183 - acc: 0.9954\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.0240 - acc: 0.9939\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.0240 - acc: 0.9936\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.0139 - acc: 0.9960\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.0121 - acc: 0.9970\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.0172 - acc: 0.9955\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.0148 - acc: 0.9961\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.0078 - acc: 0.9974\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.0067 - acc: 0.9981\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 128)          640128    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 771,841\n",
      "Trainable params: 771,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "63/63 [==============================] - 2s 33ms/step - loss: 0.8137 - acc: 0.8705\n",
      "Loss:  0.8136658668518066\n",
      "Accuracy:  0.8705000281333923\n"
     ]
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE+1, output_dim=EMBEDDING_LENGTH, input_length=MAX_SENTENCE_LENGTH),\n",
    "    tf.keras.layers.LSTM(\n",
    "        units=EMBEDDING_LENGTH, \n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "    ),\n",
    "\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4, seed=random.randint(0,15)),\n",
    "    \n",
    "    layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "\n",
    "model.fit(X_train, y_train,epochs=QNT_EPOCAS_TREINO)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss, accuracy = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# b)\n",
    "Usando as mesmas seguintes configurações para os exercicios desta lista e da lista 3:\n",
    "- VOCAB_SIZE = 5000\n",
    "- MAX_SENTENCE_LENGTH = 75\n",
    "- EMBEDDING_LENGTH = 128\n",
    "- QNT_EPOCAS_TREINO = 15\n",
    "\n",
    "\n",
    "Enquanto na lista 3 tivemos os seguintes valores:\n",
    "- Loss:  1.1264379024505615\n",
    "- Accuracy:  0.8845000267028809\n",
    "\n",
    "\n",
    "E agora usando o LSTM conseguimos:\n",
    "- Loss:  0.31173521280288696\n",
    "- Accuracy:  0.8794999718666077\n",
    "\n",
    "Apesar da Acurácia se apresentar menor com o LSTM, o LOSS foi bem menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='blue'>Questão 4 </font>\n",
    "\n",
    "a) Defina, compile, treine e avalie seu modelo, utilize camadas [GRU](https://keras.io/api/layers/recurrent_layers/gru/).\n",
    "Atenção a dimensão do input da camada de embedding, lembre se que < OOV > e < PAD > possuem seus próprios tokens.\n",
    " \n",
    " \n",
    " \n",
    "b) Como foi a performance desta rede em comparação a da lista 3?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Sua resposta aqui </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.5683 - acc: 0.7461\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.5651 - acc: 0.7530\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.5517 - acc: 0.7551\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.2751 - acc: 0.8947\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.1726 - acc: 0.9422\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.1240 - acc: 0.9626\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.0879 - acc: 0.9765\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 21s 86ms/step - loss: 0.0649 - acc: 0.9830\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0521 - acc: 0.9870\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0469 - acc: 0.9881\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0344 - acc: 0.9912\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0272 - acc: 0.9940\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0266 - acc: 0.9937\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0255 - acc: 0.9939\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 22s 87ms/step - loss: 0.0150 - acc: 0.9967\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.0171 - acc: 0.9955\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0149 - acc: 0.9960\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 22s 86ms/step - loss: 0.0114 - acc: 0.9970\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0112 - acc: 0.9972\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 22s 87ms/step - loss: 0.0090 - acc: 0.9976\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.0082 - acc: 0.9976\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 22s 86ms/step - loss: 0.0076 - acc: 0.9971\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0059 - acc: 0.9980\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 22s 86ms/step - loss: 0.0053 - acc: 0.9980\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 21s 86ms/step - loss: 0.0207 - acc: 0.9949\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.0285 - acc: 0.9899\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0083 - acc: 0.9971\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.0046 - acc: 0.9984\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.0047 - acc: 0.9982\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.0040 - acc: 0.9985\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0031 - acc: 0.9987\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 23s 91ms/step - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.0032 - acc: 0.9987\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 28s 114ms/step - loss: 0.0024 - acc: 0.9991\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.0023 - acc: 0.9990\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.0031 - acc: 0.9989\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.0026 - acc: 0.9990\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.0176 - acc: 0.9949\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 22s 89ms/step - loss: 0.0081 - acc: 0.9967\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 23s 91ms/step - loss: 0.0037 - acc: 0.9985\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.0033 - acc: 0.9985\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.0022 - acc: 0.9989\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0024 - acc: 0.9990\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0025 - acc: 0.9989\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.0023 - acc: 0.9991\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.0021 - acc: 0.9990\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 22s 89ms/step - loss: 0.0021 - acc: 0.9990\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0024 - acc: 0.9990\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.0020 - acc: 0.9992\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 128)          640128    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 739,329\n",
      "Trainable params: 739,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 1.0814 - acc: 0.8650\n",
      "Loss:  1.081394076347351\n",
      "Accuracy:  0.8650000095367432\n"
     ]
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE+1, output_dim=EMBEDDING_LENGTH, input_length=MAX_SENTENCE_LENGTH),\n",
    "    tf.keras.layers.GRU(\n",
    "        units=EMBEDDING_LENGTH, \n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "    ),\n",
    "    layers.Dropout(0.4, seed=random.randint(0,15)),\n",
    "    layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "\n",
    "model.fit(X_train, y_train,epochs=QNT_EPOCAS_TREINO)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss, accuracy = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# b)\n",
    "Usando as mesmas seguintes configurações para os exercicios desta lista e da lista 3:\n",
    "- VOCAB_SIZE = 5000\n",
    "- MAX_SENTENCE_LENGTH = 75\n",
    "- EMBEDDING_LENGTH = 128\n",
    "- QNT_EPOCAS_TREINO = 15\n",
    "\n",
    "\n",
    "Enquanto na lista 3 tivemos os seguintes valores:\n",
    "- Loss:  1.1264379024505615\n",
    "- Accuracy:  0.8845000267028809\n",
    "\n",
    "\n",
    "E agora usando o GRU conseguimos:\n",
    "- Loss:  0.6627704501152039\n",
    "- Accuracy:  0.8634999990463257\n",
    "\n",
    "Apesar da Acurácia se apresentar menor com o GRU, o LOSS foi menor, mas não foi menor do que o LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Bi-direcionais\n",
    "#  <font color='blue'>Questão 5 </font>\n",
    "\n",
    "a) Defina, compile, treine e avalie um novo modelo que utilize contexto em ambas as direções usando a camada [`Bidirectional()`](https://keras.io/api/layers/recurrent_layers/bidirectional/), seja com camadas GRU ou LSTM.\n",
    "\n",
    "\n",
    "b) Como foi sua performance em relação as questões anteriores com contexto unidirecional?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'> Sua resposta aqui </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 3475491520512.0000 - acc: 0.7732\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.4291 - acc: 0.7966\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 83.0356 - acc: 0.8357\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 705388.2500 - acc: 0.8592\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 153768.4688 - acc: 0.8952\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 1657.9614 - acc: 0.9095\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 1687.5179 - acc: 0.9042\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 481.9288 - acc: 0.8942\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 1159.1874 - acc: 0.9041\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 133.8130 - acc: 0.9121\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 135.7016 - acc: 0.9085\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 1.6112 - acc: 0.9092\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 0.2416 - acc: 0.9189\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 96.7704 - acc: 0.9202\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.8475 - acc: 0.9190\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.1892 - acc: 0.9251\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.1791 - acc: 0.9280\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.2245 - acc: 0.9296\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.1685 - acc: 0.9330\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 2.7863 - acc: 0.9325\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 407.7960 - acc: 0.9311\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.1726 - acc: 0.9300\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.1676 - acc: 0.9312\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.1718 - acc: 0.9366\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 62.3927 - acc: 0.9372\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 24.4155 - acc: 0.9412\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.1449 - acc: 0.9434\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.1406 - acc: 0.9472\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.5511 - acc: 0.9479\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.1344 - acc: 0.9475\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.1325 - acc: 0.9489\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.1295 - acc: 0.9514\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.1851 - acc: 0.9522\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 11284.7480 - acc: 0.9477\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 436732768.0000 - acc: 0.9114\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 295.9867 - acc: 0.8122\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 323.9788 - acc: 0.8615\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 104.4592 - acc: 0.8632\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 110.6009 - acc: 0.8620\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 6817.9082 - acc: 0.8877\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 2181.7124 - acc: 0.9059\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 60.9018 - acc: 0.9155\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 65.7476 - acc: 0.8982\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 59.2002 - acc: 0.9025\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 38.4099 - acc: 0.9009\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 56.0381 - acc: 0.9144\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 42.4552 - acc: 0.9282\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 63.7095 - acc: 0.9299\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 88.0792 - acc: 0.9374\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 37.5601 - acc: 0.9340\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 128)          640128    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 903,553\n",
      "Trainable params: 903,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "63/63 [==============================] - 3s 43ms/step - loss: 7.9610 - acc: 0.8695\n",
      "Loss:  7.960960388183594\n",
      "Accuracy:  0.8694999814033508\n"
     ]
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE+1, output_dim=EMBEDDING_LENGTH, input_length=MAX_SENTENCE_LENGTH),\n",
    "\n",
    "    # Camada Bidirecional\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(units=EMBEDDING_LENGTH, activation=\"tanh\", recurrent_activation=\"sigmoid\"),\n",
    "        backward_layer=tf.keras.layers.LSTM(units=EMBEDDING_LENGTH, activation=\"relu\", recurrent_activation=\"sigmoid\", go_backwards=True)\n",
    "    ),   \n",
    "    \n",
    "    layers.Dropout(0.4, seed=random.randint(0,15)),\n",
    "    layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"acc\"])\n",
    "\n",
    "model.fit(X_train, y_train,epochs=QNT_EPOCAS_TREINO)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "loss, accuracy = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) \n",
    "Usando as mesmas seguintes configurações para os exercicios desta lista e da lista 3:\n",
    "- VOCAB_SIZE = 5000\n",
    "- MAX_SENTENCE_LENGTH = 75\n",
    "- EMBEDDING_LENGTH = 128\n",
    "- QNT_EPOCAS_TREINO = 15\n",
    "\n",
    "\n",
    "Enquanto na lista 3 tivemos os seguintes valores:\n",
    "- Loss:  1.1264379024505615\n",
    "- Accuracy:  0.8845000267028809\n",
    "\n",
    "\n",
    "E agora usando o GRU conseguimos:\n",
    "- Loss:  0.6583318114280701\n",
    "- Accuracy:  0.8669999837875366\n",
    "\n",
    "Neste exemplo a Acurácia também se apresentou menor com relação ao exercicio da lista 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
